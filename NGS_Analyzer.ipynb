{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # operating system interface (e.g. file paths, directories)\n",
    "import glob  # file pattern matching (e.g. *.txt)\n",
    "from Bio.Seq import translate as t  # translate nucleotide to amino acid, alias as t\n",
    "from Bio.Seq import Seq  # Biopython object for working with biological sequences\n",
    "from multiprocessing import get_context  # parallel processing with context control\n",
    "import timeit  # timing execution of small code snippets\n",
    "import pandas as pd  # data analysis and manipulation library\n",
    "from itertools import product  # Cartesian product of input iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(y_n):\n",
    "    '''\n",
    "    converts the Sample_x_merged.fastq file to a .fasta file\n",
    "    if a .fasta file is found in the directory it is removed.\n",
    "    '''\n",
    "    create_fasta = True\n",
    "    fastas = glob.glob('*.fasta')\n",
    "    if len(fastas) > 0:\n",
    "        for a in fastas:\n",
    "            os.system('rm {}'.format(a))\n",
    "            create_fasta = True\n",
    "            # else:\n",
    "            #     create_fasta = False\n",
    "            #     print('fasta already exists, not overwriting.')\n",
    "    if create_fasta:\n",
    "        for f in glob.glob('*extended*.fastq'):\n",
    "            os.system(f\"paste - - - - < {f} | cut -f 1,2 | sed 's/^@/>/' | tr '\\t' '\\n' > {f.split('.fastq')[0]}.fasta\")\n",
    "            print('{} successfully converted to .fasta'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fasta_line(line):\n",
    "    line = line.split('\\n')[0]\n",
    "    seq_num = 0\n",
    "    dna_lines = 0\n",
    "    \n",
    "    ### NEED TO MODIFY\n",
    "    START_AA_num = 191 ## CHANGE TO 1st AA \n",
    "    Lib_Mut_Set = [201, 246, 247] #Positions with encoded mutations\n",
    "    \n",
    "    WT = 'GGAACTGTCTCAATCCAGGGAAACGCGTCTGAGATGGAGGCGGCGTTTGCAGTAGTGCCCCACTTTTTAAAGTTACCTATCAAGATGTTGATGGGGGACCACGAGGGGATGAGTCCTTCTCTGACGCACTTAACGGGCCATTTTTTCGGAGTCTACGATGGTCACGGAGGCAGCAAAGTGGCAGATTACTGTCGTGAC'\n",
    "    WT_AA_SEQ = t(WT[0:]) #Translation is in 1st reading frame, ends early (2 extra bp)\n",
    "    WT_AA_SEQ_resi = list(range(len(WT_AA_SEQ)))\n",
    "    mutated_pos_set = [m - START_AA_num for m in Lib_Mut_Set]\n",
    "    \n",
    "    for i in reversed(mutated_pos_set): #reverse since .pop() method removes member and is not smart enough to remember\n",
    "        WT_AA_SEQ_resi.pop(i)\n",
    "   \n",
    "    E201 = set('AGILPVFWYDERHKSTCMNQ')\n",
    "    G246 = set('AGILPVFWYDERHKSTCMNQ')\n",
    "    G247 = set('AGILPVFWYDERHKSTCMNQ')\n",
    "    Mut_Sets = [E201, G246, G247]\n",
    "    seq_dict = {}\n",
    "    mutation_dict = {}\n",
    "\n",
    "    if not line.startswith('>'):\n",
    "        seq = line\n",
    "        dna_lines += 1\n",
    "        if len(seq) == 198 and 'N' not in seq:\n",
    "            aa_seq = t(seq[0:])\n",
    "# =============================================================================\n",
    "#### ONLY NEEDED IF Amplicon in reverse; sometimes they merge in reverse\n",
    "            if not aa_seq.startswith('GTV'):\n",
    "                seq_class = Seq(seq)\n",
    "                aa_seq = str(t(str(seq_class.reverse_complement())[0:]))\n",
    "# =============================================================================\n",
    "                ### DONE MODIFYING?\n",
    "\n",
    "            WT_MATCH = [1 for i in WT_AA_SEQ_resi if aa_seq[i]!=WT_AA_SEQ[i]] #creates a vector of \"1\" for each sequence with mutations from WT sequence, excluding the library mutants\n",
    "            if WT_MATCH.count(1)<2: #Decrease for \"in design\" only\n",
    "                if '*' not in aa_seq:\n",
    "                    MUT_MATCH = [1 for i,j in enumerate(mutated_pos_set) if aa_seq[j] not in(Mut_Sets[i])] #creates a vector of \"1\" for each sequence with non-encoded library mutants\n",
    "                    if MUT_MATCH.count(1)<1: #Decrease for \"in design\" only\n",
    "                        seq_num += 1\n",
    "                        seq_dict[aa_seq] = 1\n",
    "                        mutation_str = ''\n",
    "                        for mutation in mutated_pos_set:\n",
    "                            mutation_str += aa_seq[mutation]\n",
    "        \n",
    "                        mutation_dict[mutation_str] = 1\n",
    "    return [seq_dict,mutation_dict, seq_num, dna_lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_dictionaries(dict_list):\n",
    "    condensed_dict = {}\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            try:\n",
    "                condensed_dict[key] += value\n",
    "            except:\n",
    "                condensed_dict[key] = value\n",
    "    return condensed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTuple(tup):\n",
    "    str = ''.join(tup)\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_fasta_mp():\n",
    "    try:\n",
    "        print('Analyzing fasta file now.')\n",
    "        for fa in glob.glob('*.fasta'):\n",
    "            lines = open(fa)\n",
    "            if os.cpu_count() > 1:\n",
    "                p2 = get_context('fork').Pool(os.cpu_count()-1)\n",
    "            else:\n",
    "                p2 = get_context('fork').Pool()\n",
    "\n",
    "            results = p2.map(process_fasta_line, lines)\n",
    "            p2.close()\n",
    "            p2.join()\n",
    "\n",
    "            s_dicts = [results[i][0] for i in range(len(results))]\n",
    "            m_dicts = [results[i][1] for i in range(len(results))]\n",
    "\n",
    "            if os.cpu_count() > 1:\n",
    "                p = get_context('fork').Pool(os.cpu_count()-1)\n",
    "            else:\n",
    "                p = get_context('fork').Pool()\n",
    "\n",
    "            seq_dict, mutation_dict = p.map(condense_dictionaries, [s_dicts, m_dicts])\n",
    "\n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "            seq_num = 0\n",
    "            dna_lines = 0\n",
    "\n",
    "            for result in results:\n",
    "                seq_num += result[2]\n",
    "                dna_lines += result[3]\n",
    "\n",
    "            print(f'Creating csv file now. {seq_num} correct reads found from {dna_lines} total sequences')\n",
    "            if 'Output' not in os.listdir():\n",
    "                os.mkdir('Output')\n",
    "\n",
    "            count_outfile = open(f\"Output/Count_{fa.split('merged_')[1].split('.extended')[0].split('S')[0].split('_')[0]}.csv\", 'w')\n",
    "            count_outfile.write(f'{seq_num}')\n",
    "            count_outfile.close()\n",
    "\n",
    "            seq_outfile = open(f\"Output/Sequences_{fa.split('merged_')[1].split('.extended')[0].split('S')[0].split('_')[0]}.csv\", 'w')\n",
    "            for sequence in sorted(seq_dict, key=seq_dict.get, reverse=True):\n",
    "                seq_outfile.write('{}, {}\\n'.format(sequence, seq_dict[sequence]/float(seq_num)))\n",
    "                seq_dict.pop(sequence)\n",
    "            seq_outfile.close()\n",
    "\n",
    "            mutation_outfile = open(f\"Output/Mutation_{fa.split('merged_')[1].split('.extended')[0].split('S')[0].split('_')[0]}.csv\", 'w')\n",
    "\n",
    "            for mutation in sorted(mutation_dict, key=mutation_dict.get, reverse=True):\n",
    "                mutation_outfile.write('{}, {}\\n'.format(mutation, mutation_dict[mutation]/float(seq_num)))\n",
    "                mutation_dict.pop(mutation)\n",
    "            mutation_outfile.close()\n",
    "            print('CSV File successfully created.')\n",
    "    except:\n",
    "        try:\n",
    "            p2.close()\n",
    "            p2.join()\n",
    "            return\n",
    "        except:\n",
    "            p.close()\n",
    "            p.join()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "folders = [ name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "merge_read_data = pd.DataFrame(columns=[\"File\",\"Total Read\", \"Merged Reads\", \"Correct Length Merged Reads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     884416-2_S8_L001_R1_001.fastq.gz\n",
      "[FLASH]     884416-2_S8_L001_R2_001.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     ./merged_884416-2_S8_L001.extendedFrags.fastq\n",
      "[FLASH]     ./merged_884416-2_S8_L001.notCombined_1.fastq\n",
      "[FLASH]     ./merged_884416-2_S8_L001.notCombined_2.fastq\n",
      "[FLASH]     ./merged_884416-2_S8_L001.hist\n",
      "[FLASH]     ./merged_884416-2_S8_L001.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           10\n",
      "[FLASH]     Max overlap:           240\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      8\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 8 combiner threads\n",
      "[FLASH] Processed 25000 read pairs\n",
      "[FLASH] Processed 50000 read pairs\n",
      "[FLASH] Processed 75000 read pairs\n",
      "[FLASH] Processed 88155 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      88155\n",
      "[FLASH]     Combined pairs:   81423\n",
      "[FLASH]     Uncombined pairs: 6732\n",
      "[FLASH]     Percent combined: 92.36%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.266 seconds elapsed\n",
      "FLASh run time for Region 2 is:  0.298680069972761\n",
      "[FLASH] Starting FLASH v1.2.11\n",
      "[FLASH] Fast Length Adjustment of SHort reads\n",
      "[FLASH]  \n",
      "[FLASH] Input files:\n",
      "[FLASH]     884416-1_S7_L001_R1_001.fastq.gz\n",
      "[FLASH]     884416-1_S7_L001_R2_001.fastq.gz\n",
      "[FLASH]  \n",
      "[FLASH] Output files:\n",
      "[FLASH]     ./merged_884416-1_S7_L001.extendedFrags.fastq\n",
      "[FLASH]     ./merged_884416-1_S7_L001.notCombined_1.fastq\n",
      "[FLASH]     ./merged_884416-1_S7_L001.notCombined_2.fastq\n",
      "[FLASH]     ./merged_884416-1_S7_L001.hist\n",
      "[FLASH]     ./merged_884416-1_S7_L001.histogram\n",
      "[FLASH]  \n",
      "[FLASH] Parameters:\n",
      "[FLASH]     Min overlap:           10\n",
      "[FLASH]     Max overlap:           240\n",
      "[FLASH]     Max mismatch density:  0.250000\n",
      "[FLASH]     Allow \"outie\" pairs:   false\n",
      "[FLASH]     Cap mismatch quals:    false\n",
      "[FLASH]     Combiner threads:      8\n",
      "[FLASH]     Input format:          FASTQ, phred_offset=33\n",
      "[FLASH]     Output format:         FASTQ, phred_offset=33\n",
      "[FLASH]  \n",
      "[FLASH] Starting reader and writer threads\n",
      "[FLASH] Starting 8 combiner threads\n",
      "[FLASH] Processed 25000 read pairs\n",
      "[FLASH] Processed 50000 read pairs\n",
      "[FLASH] Processed 75000 read pairs\n",
      "[FLASH] Processed 80754 read pairs\n",
      "[FLASH]  \n",
      "[FLASH] Read combination statistics:\n",
      "[FLASH]     Total pairs:      80754\n",
      "[FLASH]     Combined pairs:   74297\n",
      "[FLASH]     Uncombined pairs: 6457\n",
      "[FLASH]     Percent combined: 92.00%\n",
      "[FLASH]  \n",
      "[FLASH] Writing histogram files.\n",
      "[FLASH]  \n",
      "[FLASH] FLASH v1.2.11 complete!\n",
      "[FLASH] 0.259 seconds elapsed\n",
      "FLASh run time for Region 1 is:  0.27190474997041747\n"
     ]
    }
   ],
   "source": [
    "# #Merge R1 and R2 together with FLASH\n",
    "for fold in folders:\n",
    "    os.chdir(fold)\n",
    "    f = glob.glob('*.fastq.gz')\n",
    "    a_time = timeit.default_timer()\n",
    "    os.system(f\"/Users/samdswift/projects/FLASH-1.2.11/flash {f[0]} {f[1]} -M 240 -o merged_{f[0].split('_R')[0]} 2>&1 | tee flash.log\") #-M is maximum merge overlap length; -m is minimum merge overlap length\n",
    "    h = glob.glob('*.hist')\n",
    "    x = pd.read_csv(h[0], sep='\\t', header=None)\n",
    "    sum_reads = x[1].sum(axis=0)\n",
    "    cor_read_length = x[1][x[0]==189].values[0]\n",
    "    with open(r'flash.log', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines: \n",
    "            if line.find('Total pairs:') !=-1:\n",
    "                total_pairs = line\n",
    "    total_reads = int(total_pairs.split(':')[-1].split(' ')[-1].split('\\n')[0])\n",
    "    merge_read_data.loc[len(merge_read_data.index)] = [fold, total_reads, sum_reads, cor_read_length]\n",
    "    print(f'FLASh run time for {fold} is: ', timeit.default_timer()-a_time)\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_884416-2_S8_L001.extendedFrags.fastq successfully converted to .fasta\n",
      "Conversion time for Region 2 is:  4.205493391025811\n",
      "merged_884416-1_S7_L001.extendedFrags.fastq successfully converted to .fasta\n",
      "Conversion time for Region 1 is:  3.401721064001322\n"
     ]
    }
   ],
   "source": [
    "#Convert the merged fastq to a fasta for easier viewing\n",
    "for fold in folders:\n",
    "    os.chdir(fold)\n",
    "    b_time = timeit.default_timer()\n",
    "    convert(glob.glob('merged*.extended*.fastq')[0])\n",
    "    print(f'Conversion time for {fold} is: ', timeit.default_timer()-b_time)\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Region 2 sequences now. \n",
      "\n",
      "Analyzing fasta file now.\n",
      "Creating csv file now. 5 correct reads found from 81423 total sequences\n",
      "CSV File successfully created.\n",
      "Time to run FASTA Frequency for Region 2 is:  3.1133816550136544\n",
      "Running Region 1 sequences now. \n",
      "\n",
      "Analyzing fasta file now.\n",
      "Creating csv file now. 70831 correct reads found from 74297 total sequences\n",
      "CSV File successfully created.\n",
      "Time to run FASTA Frequency for Region 1 is:  3.6625504590338096\n"
     ]
    }
   ],
   "source": [
    "for fold in folders:\n",
    "    os.chdir(fold)\n",
    "    c_time = timeit.default_timer()\n",
    "    print(f'Running {fold} sequences now. \\n')\n",
    "    get_freq_fasta_mp()\n",
    "    print(f'Time to run FASTA Frequency for {fold} is: ' , timeit.default_timer()-c_time)\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile all         \n",
    "all_muts_df = pd.DataFrame(columns=[0,1])\n",
    "all_seqs_df = pd.DataFrame(columns=[0,1])\n",
    "for fold in folders:\n",
    "    name = fold\n",
    "    os.chdir(fold)\n",
    "    os.chdir('Output')\n",
    "    count_file = glob.glob('Count*.csv')\n",
    "    count_df = pd.read_csv(count_file[0], header=None)\n",
    "    count = count_df.iloc[0,0]\n",
    "    mut_file = glob.glob('Mut*')\n",
    "    mut_df = pd.read_csv(mut_file[0], header=None)\n",
    "    mut_df[fold+' Count']=mut_df[1]*count\n",
    "    mut_df = mut_df.drop(1, axis=1)\n",
    "    \n",
    "    seq_file = glob.glob('Seq*')\n",
    "    seq_df = pd.read_csv(seq_file[0], header=None)\n",
    "    seq_df[fold+' Count']=seq_df[1]*count\n",
    "    seq_df = seq_df.drop(1, axis=1)\n",
    "    \n",
    "    all_muts_df = pd.merge(all_muts_df, mut_df, on=0, how='outer')\n",
    "    all_seqs_df = pd.merge(all_seqs_df, seq_df, on=0, how='outer')\n",
    "    os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_read_data_2 = pd.DataFrame()\n",
    "merge_read_data_2['% Merge Reads'] = merge_read_data['Merged Reads']/ merge_read_data['Total Read']\n",
    "merge_read_data_2['% Correct Length'] = merge_read_data['Correct Length Merged Reads']/merge_read_data['Merged Reads']    \n",
    "merge_read_data_full = pd.merge(merge_read_data, merge_read_data_2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV (update with your actual filename)\n",
    "df = pd.read_csv('/Users/samdswift/Desktop/amp2run/Region 2/Output/Mutation_884416-2.csv', header=None, names=['Mutant', 'Frequency'])\n",
    "\n",
    "# Define WT sequence\n",
    "wt = df.iloc[0, 0]\n",
    "\n",
    "# Prepare table: rows = mutant AA, columns = position\n",
    "positions = list(range(1, len(wt)+1))\n",
    "mutants = sorted(set(''.join(df['Mutant'])))\n",
    "table = pd.DataFrame('', index=mutants, columns=positions)\n",
    "\n",
    "# Populate table with single mutants only\n",
    "for _, row in df.iterrows():\n",
    "    seq = row['Mutant']\n",
    "    freq = row['Frequency']\n",
    "    \n",
    "    # Count mutations\n",
    "    diffs = [(i, wt[i], seq[i]) for i in range(len(wt)) if wt[i] != seq[i]]\n",
    "    \n",
    "    if len(diffs) == 1:  # Only consider single mutants\n",
    "        i, _, mut = diffs[0]\n",
    "        table.at[mut, i+1] = f\"{freq:.15%}\"\n",
    "\n",
    "# Display table\n",
    "table.to_csv(\"single_mutation_table.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
